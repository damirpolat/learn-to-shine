scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#combine features and perform tuning
tuneModel = function(data, learner, rs.iters, par.set, scenario, metrics) {
#taken mostly from server script and adjusted for this model
des = ParamHelpers::generateRandomDesign(rs.iters, par.set, trafo = TRUE)
des.list = ParamHelpers::dfRowsToList(des, par.set)
parallelStartMulticore()
ys = parallelMap(function(x) {
par10 = try({
learner = setHyperPars(learner, par.vals = x)
p = regressionCombined(learner = learner, data = data, measure = scenario$desc$performance_measures,
metrics = metrics, scenario = scenario)
ldf = fixFeckingPresolve(scenario, data)
par10 = mean(parscores(ldf, p, timeout = timeout))
messagef("[Tune]: %s : par10 = %g", ParamHelpers::paramValueToString(par.set, x), par10)
return(par10)
})
if(inherits(par10, "try-error")) {
par10 = NA
}
return(par10)
}, des.list, simplify = TRUE, level = "mlr.tuneParams")
parallelStop()
best.i = getMinIndex(ys)
best.parvals = des.list[[best.i]]
messagef("[Best]: %s : par10 = %g", ParamHelpers::paramValueToString(par.set, best.parvals), ys[best.i])
return(best.parvals)
}
options = list(make_option(c("--scenario"), type="character", default=NULL,
help="hyperparameters for model", metavar="character"),
make_option(c("--metrics"), type="character", default=NULL,
help="metrics for scenario", metavar="character"),
make_option(c("--save"), type="character", default=NULL,
help="directory for saving results", metavar="character"),
make_option(c("--fold"), type = "character", default = NULL,
help="fold number", metavar="character"),
make_option(c("--solver"), type = "character", default = NULL,
help="solver number", metavar="character"))
opt_parser = OptionParser(option_list=options)
opt = parse_args(opt_parser)
opt$scenario = "./modeling-algorithmic-performance/Models/aslib_scenarios/SAT11-INDU/"
opt$metrics = "./modeling-algorithmic-performance/Models/sat11indu_model/solvers_metrix.csv"
opt$fold = "1"
opt$solver = "1"
fold.n = as.integer(opt$fold)
solver.n = as.integer(opt$solver)
fold.n
solver.n
#convert data to llama frame
inputData = input(opt$scenario, opt$metrics)
llama.cv = convertToLlama(inputData$scenario)
#specify parameter search set
par.set = makeParamSet(
makeIntegerParam("ntree", lower = 10, upper = 200),
makeIntegerParam("mtry", lower = 1, upper = 30)
)
n.inner.folds = 3L  #Number of cross-validation folds for inner CV in hyperparameter tuning
rs.iters = 250L  #Number of iterations for random search hyperparameter tuning
rs.iters = 1L
#learner with default parameters
learner = makeImputeWrapper(learner = setHyperPars(makeLearner("regr.randomForest")),
classes = list(numeric = imputeMean(), integer = imputeMean(), logical = imputeMode(),
factor = imputeConstant("NA"), character = imputeConstant("NA")))
cutoff = inputData$scenario$desc$algorithm_cutoff_time
timeout = if(inputData$scenario$desc$performance_type[[1L]] == "runtime" && !is.na(cutoff)) {
cutoff
} else {
NULL
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n)
#train model with nested cross-validation and tuning
trainModel = function(llama.cv, learner, metrics, n.inner.folds, timeout, rs.iters, par.set, scenario, fold.n, solver.id) {
#train on each train split
rest.model = vector("list")
solver = as.character(metrics$algorithm[[solver.id]])
set.seed(1, "L'Ecuyer")
scenarioLeaveOne = scenario
#remove solver from description
scenarioLeaveOne$desc$algorithms_deterministic = subset(scenarioLeaveOne$desc$algorithms_deterministic, scenarioLeaveOne$desc$algorithms_deterministic != solver)
scenarioLeaveOne$desc$algorithms_stochastic = subset(scenarioLeaveOne$desc$algorithms_stochastic, scenarioLeaveOne$desc$algorithms_stochastic != solver)
scenarioLeaveOne$desc$metainfo_algorithms[[solver]] = NULL
#remove solver from runs and status
scenarioLeaveOne$algo.runs = subset(scenarioLeaveOne$algo.runs, scenarioLeaveOne$algo.runs$algorithm != solver)
scenarioLeaveOne$algo.runstatus[[solver]] = NULL
#remove solver from metrics
metricsLeaveOne = subset(metrics, metrics$algorithm != solver)
llama.cv = convertToLlamaCVFolds(scenarioLeaveOne)
outer.preds = lapply(fold.n:fold.n, function(k) {
#data for inner cv split
inner.data = llama.cv
inner.data$data = llama.cv$data[llama.cv$train[[k]],]
inner.data$train = NULL
inner.data$test = NULL
inner.newdata = cvFolds(inner.data, nfolds = n.inner.folds, stratify = FALSE)
data = inner.newdata
#obtain best hyperparameters
best.parvals = tuneModel(data, learner, rs.iters, par.set, scenarioLeaveOne, metricsLeaveOne)
#make test and train data split
data = llama.cv
data$train = list(llama.cv$train[[k]])
data$test = list(llama.cv$test[[k]])
learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.id)
#train model with nested cross-validation and tuning
trainModel = function(llama.cv, learner, metrics, n.inner.folds, timeout, rs.iters, par.set, scenario, fold.n, solver.id) {
#train on each train split
rest.model = vector("list")
solver = as.character(metrics$algorithm[[solver.id]])
set.seed(1, "L'Ecuyer")
scenarioLeaveOne = scenario
#remove solver from description
scenarioLeaveOne$desc$algorithms_deterministic = subset(scenarioLeaveOne$desc$algorithms_deterministic, scenarioLeaveOne$desc$algorithms_deterministic != solver)
scenarioLeaveOne$desc$algorithms_stochastic = subset(scenarioLeaveOne$desc$algorithms_stochastic, scenarioLeaveOne$desc$algorithms_stochastic != solver)
scenarioLeaveOne$desc$metainfo_algorithms[[solver]] = NULL
#remove solver from runs and status
scenarioLeaveOne$algo.runs = subset(scenarioLeaveOne$algo.runs, scenarioLeaveOne$algo.runs$algorithm != solver)
scenarioLeaveOne$algo.runstatus[[solver]] = NULL
#remove solver from metrics
metricsLeaveOne = subset(metrics, metrics$algorithm != solver)
llama.cv = convertToLlamaCVFolds(scenarioLeaveOne)
outer.preds = lapply(fold.n:fold.n, function(k) {
#data for inner cv split
inner.data = llama.cv
inner.data$data = llama.cv$data[llama.cv$train[[k]],]
inner.data$train = NULL
inner.data$test = NULL
inner.newdata = cvFolds(inner.data, nfolds = n.inner.folds, stratify = FALSE)
data = inner.newdata
#obtain best hyperparameters
best.parvals = tuneModel(data, learner, rs.iters, par.set, scenarioLeaveOne, metricsLeaveOne)
#make test and train data split
data = llama.cv
data$train = list(llama.cv$train[[k]])
data$test = list(llama.cv$test[[k]])
learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.id)
solver.id
#train model with nested cross-validation and tuning
trainModel = function(llama.cv, learner, metrics, n.inner.folds, timeout, rs.iters, par.set, scenario, fold.n, solver.n) {
#train on each train split
rest.model = vector("list")
solver = as.character(metrics$algorithm[[solver.n]])
set.seed(1, "L'Ecuyer")
scenarioLeaveOne = scenario
#remove solver from description
scenarioLeaveOne$desc$algorithms_deterministic = subset(scenarioLeaveOne$desc$algorithms_deterministic, scenarioLeaveOne$desc$algorithms_deterministic != solver)
scenarioLeaveOne$desc$algorithms_stochastic = subset(scenarioLeaveOne$desc$algorithms_stochastic, scenarioLeaveOne$desc$algorithms_stochastic != solver)
scenarioLeaveOne$desc$metainfo_algorithms[[solver]] = NULL
#remove solver from runs and status
scenarioLeaveOne$algo.runs = subset(scenarioLeaveOne$algo.runs, scenarioLeaveOne$algo.runs$algorithm != solver)
scenarioLeaveOne$algo.runstatus[[solver]] = NULL
#remove solver from metrics
metricsLeaveOne = subset(metrics, metrics$algorithm != solver)
llama.cv = convertToLlamaCVFolds(scenarioLeaveOne)
outer.preds = lapply(fold.n:fold.n, function(k) {
#data for inner cv split
inner.data = llama.cv
inner.data$data = llama.cv$data[llama.cv$train[[k]],]
inner.data$train = NULL
inner.data$test = NULL
inner.newdata = cvFolds(inner.data, nfolds = n.inner.folds, stratify = FALSE)
data = inner.newdata
#obtain best hyperparameters
best.parvals = tuneModel(data, learner, rs.iters, par.set, scenarioLeaveOne, metricsLeaveOne)
#make test and train data split
data = llama.cv
data$train = list(llama.cv$train[[k]])
data$test = list(llama.cv$test[[k]])
learner = setHyperPars(learner, par.vals = best.parvals)
return(regressionCombined(learner = learner, data = data, measure = scenarioLeaveOne$desc$performance_measures,
metrics = metricsLeaveOne))
})
rest.model$models = lapply(outer.preds, function(x) { x$models } )
rest.model$predictions = rbindlist(lapply(outer.preds, function(x) { x$predictions }))
#set up data for left out solver
scenarioLeftSolver = scenario
#leave one solver in description
scenarioLeftSolver$desc$algorithms_deterministic = subset(scenarioLeftSolver$desc$algorithms_deterministic, scenarioLeftSolver$desc$algorithms_deterministic == solver)
scenarioLeftSolver$desc$algorithms_stochastic = subset(scenarioLeftSolver$desc$algorithms_stochastic, scenarioLeftSolver$desc$algorithms_stochastic == solver)
scenarioLeftSolver$desc$metainfo_algorithms = scenarioLeftSolver$desc$metainfo_algorithms[solver]
#leave one solver in runs and status
scenarioLeftSolver$algo.runs = subset(scenarioLeftSolver$algo.runs, scenarioLeftSolver$algo.runs$algorithm == solver)
scenarioLeftSolver$algo.runstatus = select(scenarioLeftSolver$algo.runstatus, c("instance_id", "repetition", solver))
#leave on solver from metrics
metricsLeftSolver = subset(metrics, metrics$algorithm == solver)
llama.cv.solver = convertToLlamaCVFolds(scenarioLeftSolver)
data.solver = llama.cv.solver
data.solver$train = list(llama.cv.solver$train[[fold.n]])
data.solver$test = list(llama.cv.solver$test[[fold.n]])
solverLeft.preds = regressionLeftSolver(model = rest.model$models, data = data.solver, train.data = llama.cv, train.metrics = metricsLeaveOne,
new.metrics = metricsLeftSolver, measure = scenarioLeftSolver$desc$performance_measures)
solver.preds = list(solverLeft = solverLeft.preds, rest.model = rest.model)
return(solver.preds)
}
#train
outer.preds = trainModel(llama.cv, learner, inputData$metrics, n.inner.folds, timeout, rs.iters, par.set, inputData$scenario, fold.n, solver.n)
outer.preds
combined_model = outer.preds$solverLeft
combined_model = outer.preds$solverLeft
test.preds = outer.preds$solverLeft$test.predictions
test.preds
test.model = outer.preds$solverLeft
test.model$train.predictions = NULL
test.model$test.predictions = NULL
test.model$predictions = test.preds
combined_model
combined_model$test.predictions
# sort predictions
if(inputData$scenario$desc$maximize[[inputData$scenario$desc$performance_measures]]) {
test.model$predictions = test.model$predictions[order(test.model$predictions$instance_id, -test.model$predictions$score)]
} else {
test.model$predictions = test.model$predictions[order(test.model$predictions$instance_id, test.model$predictions$score)]
}
combined_model = list(test.model = test.model)
rest.model = outer.preds$rest.model$models
names = inputData$metrics$algorithm[[solver.n]]
rest.model = setNames(rest.model, names)
rest.model$predictions = outer.preds$rest.model$predictions
rest.model$predictions
combined_model$test.model$predictions
combined_model$test.model
getwd()
opt$save = "./manual-folds-manual-solvers/"
save(combined_model, file = sprintf("%s/proposed_leave_one_out_%s_%s_%s.RData", opt$save, inputData$scenario$desc$scenario_id, solver.n, fold.n))
save(rest.model, file = sprintf("%s/proposed_leave_one_out_rest_%s_%s_%s.RData", opt$save, inputData$scenario$desc$scenario_id, solver.n, fold.n))
combined_model$test.model$predictions
unqiue(combined_model$test.model$predictions$algorithm)
unique(combined_model$test.model$predictions$algorithm)
rest.model$predictions
library(shiny); runApp('learn-to-shine/compareSelectors.R')
library(shiny)
library(mlr)
library(llama)
library(aslib)
runApp('learn-to-shine/compareSelectors.R')
library(shiny)
library(mlr)
library(llama)
library(aslib)
library(ggplot2)
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
library(plotly)
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
runApp('learn-to-shine/compareSelectors.R')
setwd("learn-to-shine/compare-selectors/")
library(shiny); runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
# compute mean mcp or gap closed
compute_metric = function(scenario, llama.cv, choice, method) {
data = fixFeckingPresolve(scenario, llama.cv)
if(method == "mcp") {
if(choice == "sbs") {
single = llama:::singleBest(llama.cv)
single = list(predictions = single)
attr(single, "hasPredictions") = TRUE
val = mean(misclassificationPenalties(data, single))
} else if(choice == "vbs") {
vbs = llama:::vbs(llama.cv)
vbs = list(predictions = vbs)
attr(vbs, "hasPredictions") = TRUE
val = mean(misclassificationPenalties(data, vbs))
}
} else if(method == "par10") {
if(choice == "sbs") {
single = llama:::singleBest(llama.cv)
single = list(predictions = single)
attr(single, "hasPredictions") = TRUE
val = mean(parscores(data, single))
} else if(choice == "vbs") {
vbs = llama:::vbs(llama.cv)
vbs = list(predictions = vbs)
attr(vbs, "hasPredictions") = TRUE
val = mean(parscores(data, vbs))
}
}
return(val)
}
runApp('compareSelectors.R')
llama.cv
sc = getCosealASScenario("SAT11-INDU")
llama.cv = trainTest(convertToLlama(sc))
m = compute_metric(sc, llama.cv, "sbs", "mcp")
m
m = compute_metric(sc, llama.cv, "vbs", "mcp")
m
d = data.frame("single best" = m)
d
d = data.frame("single\ best" = m)
d
d = data.frame(sprintf("single best") = m)
d
summary(1:10)
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
summary(1)
runApp('compareSelectors.R')
m
summary(m)
runApp('compareSelectors.R')
runApp('compareSelectors.R')
summary(letters)
runApp('compareSelectors.R')
runApp('compareSelectors.R')
summary(sc)
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
class()
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
d
col = "a"
d[, col] = 1
d
col2 = "b"
d[, col, col2] = 1
d
runApp('compareSelectors.R')
d = data.frame("a" = c(1, 2))
d
runApp('compareSelectors.R')
runApp('compareSelectors.R')
d
row.names(d)
row.names(d) = c("gap closed")
row.names(d) = c("gap closed", "gap")
d
runApp('compareSelectors.R')
runApp('compareSelectors.R')
summary(d)
d
d = data.frame("a" = 1:2)
d
d = data.frame("a" = 1:2, row.names = FALSE)
d
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
install.packages("fnn")
runApp('compareSelectors.R')
# compute percentage of closed gap
compute_gap =  function(model_val, vbs_val, sbs_val) {
return(round(1 - (model_val - vbs_val) / (sbs_val - vbs_val), 2))
}
vars = c(1.0, 2.0)
vars
virtual_mcp
mcp_single
single_mcp
map(vars, compute_gap)
lapply(vars, compute_gap)
library(map)
library(purrr)
?map
lapply(vars, compute_gap, 1.0, 3.0)
map(vars, compute_gap, 1.0, 3.0)
ret = map(vars, compute_gap, 1.0, 3.0)
ret[[1]]
ret[1]
ret[[1]]
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('experimentUI.R')
runApp('experimentUI.R')
?fileInput
runApp('experimentUI.R')
runApp('experimentUI.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
install.packages("shinyFiles")
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
runApp('experiments/experimentUI.R')
?shinyDirButton
runApp('experiments/experimentUI.R')
runApp('compareSelectors.R')
?shinyDirChoose
runApp('experiments/experimentUI.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
?observeEvent
runApp('compareSelectors.R')
runApp('compareSelectors.R')
runApp('experiments/experimentUI.R')
runApp('compareSelectors.R')
runApp('compareSelectors.R')
